### 学习点滴记录

- 2020年05月14日
  - 如何入门Keras？https://www.zhihu.com/question/51767944 
  - 带你少走弯路：强烈推荐的Keras快速入门资料和翻译（可下载）https://blog.csdn.net/lc013/article/details/102739350
    - https://morvanzhou.github.io/tutorials/machine-learning/keras/ 莫凡Python
  - https://tensorflow.google.cn/guide/keras 谷歌官方教程
  - [Keras 中文文档](https://keras.io/zh/)



### 知识路线汇总

- 关于深度学习框架 TensorFlow、Theano 和 Keras https://www.cnblogs.com/shenxiaolin/p/11121152.html

- pip 如何国内源安装keras等

  ```python
  pip install easydict -i https://pypi.tuna.tsinghua.edu.cn/simple/ 
  pip install keras==2.0.8  -i https://pypi.tuna.tsinghua.edu.cn/simple/  
  pip install Cython opencv-python -i https://pypi.tuna.tsinghua.edu.cn/simple/ 
  pip install matplotlib -i https://pypi.tuna.tsinghua.edu.cn/simple/ 
  pip install -U pillow -i https://pypi.tuna.tsinghua.edu.cn/simple/
  pip install  h5py lmdb mahotas -i https://pypi.tuna.tsinghua.edu.cn/simple/
  ```

  pip3 安装python3版本

- Anaconda|Anaconda与ROS共存问题

  - https://blog.csdn.net/qq_33521754/article/details/78829212
  
  - 解决ros和anaconda python 版本冲突问题  https://blog.csdn.net/baidu_41703242/article/details/83749594   
  
  - 若anaconda和ros的python版本不同,则往往会出现版本冲突问题,导致运行roscore报错.提示缺少某种依赖,然而这时去安装这个依赖会提示已安装. 
  
  - 解决Ubuntu16.04中同时使用Anaconda 3和ROS默认Python版本矛盾的问题
  
  - 原因：ROS默认Python版本为2.x，Anaconda 3将系统的Python设置为3.x。这样编译ROS工作空间中的某些包（例如tf）时，就会因为Python版本报错。
  
    解决方法1： 修改/usr/bin中的python连接，/usr/bin中的python本质是同一文件夹下的python2.x或python3.x的符号链接，因此只要修改连接指向，就可以修改python默认版本。
  
    cd /usr/bin
    ls python* #查看现有python版本
    sudo rm python #此处要输密码
    sudo ln -s python2.x python # 创立新的符号链接
    解决方法2： 使用Anaconda创建新的环境，制定python版本为2.x。
    以2.7为例：
  
    conda create -n py27 python=2.7
    source activate py27
    source /opt/ros/kinetic/setup.bash
    source /home/PATH_TO_YOUR_WORKSPACE/devel/setup.bash
    这样就可以愉快地编译ROS包，正常使用ROS了。
  
    链接：https://www.jianshu.com/p/70304cfe44d4
    来源：简书
  
  - 另外合理的解决方案: https://zhuanlan.zhihu.com/p/137908448 
  
  -  **在最后一行加入如下代码：**
  
    conda deactivate
    
  - Tensorflow  1.X 安装:
  
    pip install --index-url https://pypi.douban.com/simple tensorflow
  
    或pip install --index-url http://mirrors.aliyun.com/pypi/simple/ tensorflow
  
     
  
    Tensorflow  2.X 安装: (豆瓣源,  注意要是  https,  有个s )
  
    pip install -i https://pypi.doubanio.com/simple/ tensorflow
  
  - anaconda添加国内源+tensorflow安装
  
    - ```cpp
      conda install -n my-py-env --channel https://mirrors.ustc.edu.cn/anaconda/pkgs/main/   tensorflow-gpu
      ```
  
    - https://www.jianshu.com/p/e6a9aa0e671b
  
    
  
  - 机器学习超参数mini-batch（小批量）与正则化
  
    ```
    1、一个epoch（周期）为所有数据完成一次前向与反向传播，iteration(迭代次数)为权重参数更新的次数，mini-batch（小批量）为一次权重参数更新所用的数据条数。迭代次数=数据总数／小批量 ，例如数据总数为2000条数据，小批量为500，那么一个周期内迭代次数=2000／500=4 ，如果是两个周期，那么迭代次数=2*（2000／500）=8
    
    2、mini-batch越大，意味着计算量越大，梯度越平滑。但不是越大越好，权衡内存，计算量，优化效果。
    
    3、mini-batch一般为2的次方，层的大小一般也为2的次方，这是由硬件特性造成的。
    
    4、使用CPU进行训练时mini-batch一般为32到256，使用GPU进行训练时mini-batch一般为32 到1024
    
    5、当你增大mini-batch时，迭代次数就会减少，那么参数更新次数也会减小，因此，要适当增加epoch的次数。
    
    6、正则化是为了避免权重参数变化太快太大，以防过拟合。
    
    7、L2更重地惩罚较大的权重，但不会将较小的权重减到0。L1对大权重的惩罚较小，但会导致许多权重被减少到0（或非常接近0），这意味着合成权重向量可以是稀疏的。
    
    原文链接：https://blog.csdn.net/bewithme/article/details/86708991
    ```
  
- 深度学习中的三种梯度下降方式：批量（batch），随机（stochastic），小批量（mini-batch）

  ```
  1，批量梯度下降法（Batch Gradient Descent） ：在更新参数时都使用所有的样本来进行更新。
  
  　　优点：全局最优解，能保证每一次更新权值，都能降低损失函数；易于并行实现。
  
  　　缺点：当样本数目很多时，训练过程会很慢。
  
  　　2，随机梯度下降法（Stochastic Gradient Descent）：在更新参数时都使用一个样本来进行更新。每一次跟新参数都用一个样本，更新很多次。如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将参数迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次，这种方式计算复杂度太高。
  
  　　优点：训练速度快；
  
  　　缺点：准确度下降，并不是全局最优；不易于并行实现。从迭代的次数上来看，随机梯度下降法迭代的次数较多，在解空间的搜索过程看起来很盲目。噪音很多，使得它并不是每次迭代都向着整体最优化方向。
  
  　　3，小批量梯度下降法（Mini-batch Gradient Descen）：在更新每一参数时都使用一部分样本来进行更新。为了克服上面两种方法的缺点，又同时兼顾两种方法的优点。
  
  　　4，三种方法使用的情况：如果样本量比较小，采用批量梯度下降算法。如果样本太大，或者在线算法，使用随机梯度下降算法。在实际的一般情况下，采用小批量梯度下降算法。
  ————————————————
  版权声明：本文为CSDN博主「xiaotao_1」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
  原文链接：https://blog.csdn.net/xiaotao_1/article/details/81031633
  ```

- 深度学习中偏移项bias的作用

  ```
  还有其他的理解就是，这里的bias是所选模型自带的固有的误差，因为选的模型必不可能是完美的，bias与数据无关，是模型自带的
  最简单的解释就是原来的模型是w1x1+w2x2+...+wnxn>=b这里的b就是bias也就是阈值，当达到某个阈值的时候，产生输出，但是为了模型的方便将b移到了左边变成w1x1+...+wnxn+(-b)>=0
  
  原文链接：https://blog.csdn.net/hy13684802853/article/details/78717475
  ```

  

- 深度学习中的dropout

  - https://blog.csdn.net/u012762419/article/details/79534085

  - 深度神经网络的训练是一件非常困难的事，涉及到很多因素，比如损失函数的非凸性导致的局部最优值、计算过程中的数值稳定性、训练过程中的过拟合等。其中，过拟合是很容易发生的现象，也是在训练DNN中必须要解决的问题。

  - 过拟合是指模型训练到一定程度后，在训练集上得到的测试误差远大于在测试集上得到的误差，如下图所示： 

  - 导致过拟合的直接原因是模型学习了太多噪声模式，并错误的将其视为数据模式；导致过拟合的主要原因有： 
    1. 训练数据集太小 
    2. 模型太复杂 
    3. 过度训练

    常用的过拟合解决方案
    知道了产生过拟合的原因，就可以制定相应的解决方案，一般而言，解决的主要方法有：增加训练数据量、减少模型的复杂度、添加正则项等。在深度学习中，以上方法都可以使用，但是dropout是一个更加高效、简单的防止过拟合的方法

  - ropout是指在训练一个大的神经网络的时候，随机“关闭”一些神经元，即把这些神经元从网络中“抹去”，这相当于在本次训练中，这些被“抹去”的神经元不参与本次训练，英文即是“dropout”的意思。

  - 打叉的神经元就是被“dropout”掉的神经元，和这些个神经元相连接的权重值也一并被“抹去”，不参与本次训练。不参与本次训练是说在当前的batch中，不参与训练，每个batch都会随机挑选神经元做dropout。

  - 随机森林”是一种不容易发生过拟合的算法，其采用的方法是“bagging”，即通过对多个树的输出结果加权平均，得出最后的结果。而每棵树都是在不同的训练集上、设置不同的参数得到的，因此是一种典型的通过多个不同参数，甚至不同类型的模型来降低方差的一种方法。

- 为什么引入非线性激励函数

  - https://blog.csdn.net/siyue0211/article/details/81017728   说明合理,解释合理

  - ReLU是什么的简称 Rectified Linear Unit (整流线性单元) https://blog.csdn.net/houhuipeng/article/details/96423888

  - 如果不用激励函数，在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你有多少层神经网络，输出的都是输入的线性组合。

    激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。

- 什么是Adam/ReLU/YOLO？这里有一份深度学习（.ai）词典

  - https://cloud.tencent.com/developer/article/1540684 	本文旨在解释深度学习的一些常用术语，尤其是吴恩达在deeplearning.ai的Coursera课程中会频繁提到的重要词汇
    - https://blog.csdn.net/sallyyoung_sh/article/details/73091029  

- virtualenv安装、使用、pip国内镜像替换---windows 0117-2020

  - https://zhuanlan.zhihu.com/p/103238402